Machine Learning

Basics
Matrix factorization
Dividing complex matrices into parts to better handle and computation ease. Three ways of performing matrix decomposition (or factorization.

LU Matrix Decomposition

The LU decomposition is for square matrices and decomposes a matrix into L and U components.

A = L . U

The LU decomposition is often used to simplify the solving of systems of linear equations, such as finding the coefficients in a linear regression, as well as in calculating the determinant and inverse of a matrix.

A variation of this decomposition that is numerically more stable to solve in practice is called the LUP decomposition, or the LU decomposition with partial pivoting.

A = P . L . U

The LU decomposition can be implemented in Python with the lu() function. More specifically, this function calculates an LPU decomposition.


QR Matrix Decomposition

The QR decomposition is for m x n matrices (not limited to square matrices) and decomposes a matrix into Q and R components.

A = Q . R

Where A is the matrix that we wish to decompose, Q a matrix with the size m x m, and R is an upper triangle matrix with the size m x n.

The QR decomposition is found using an iterative numerical method that can fail for those matrices that cannot be decomposed, or decomposed easily.

Like the LU decomposition, the QR decomposition is often used to solve systems of linear equations, although is not limited to square matrices.

The QR decomposition can be implemented in NumPy using the qr() function. By default, the function returns the Q and R matrices with smaller or ‘reduced’ dimensions that is more economical. We can change this to return the expected sizes of m x m for Q and m x n for R by specifying the mode argument as ‘complete’, although this is not required for most applications.


Cholesky Decomposition

The Cholesky decomposition is for square symmetric matrices where all eigenvalues are greater than zero, so-called positive definite matrices.

For our interests in machine learning, we will focus on the Cholesky decomposition for real-valued matrices and ignore the cases when working with complex numbers.

The decomposition is defined as follows:

A = L . L^T

Where A is the matrix being decomposed, L is the lower triangular matrix and L^T is the transpose of L.

The decompose can also be written as the product of the upper triangular matrix, for example:

A = U^T . U

The Cholesky decomposition is used for solving linear least squares for linear regression, as well as simulation and optimization methods.

When decomposing symmetric matrices, the Cholesky decomposition is nearly twice as efficient as the LU decomposition and should be preferred in these cases.

While symmetric, positive definite matrices are rather special, they occur quite frequently in some applications, so their special factorization, called Cholesky decomposition, is good to know about. When you can use it, Cholesky decomposition is about a factor of two faster than alternative methods for solving linear equations.


Singular Value Decomposition (SVD)

Can be use for any type of matrices (square or non-square).
Approximates decomposition when m < n and m > n.

A = U . S . VT

A - Matrix and U is m x m square matrix, S is rectangular matrix of shape m x n and VT is square matrix of shape n x n

Can be used for data compression data approximation can be controlled by singular value `sigma`.

Inverse of a SVD also approximates to nearly A inverse.


Reference :: 
Matrix Decomposition LUP / QR / Cholesky => https://machinelearningmastery.com/introduction-to-matrix-decompositions-for-machine-learning/
Matrix Factorization SVD => https://towardsdatascience.com/understanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33

